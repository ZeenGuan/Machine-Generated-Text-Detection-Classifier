{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d64b9d-3b01-44dd-bdf7-b7862fa252ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25870385-bdc1-4bcf-88df-5531fe8a0a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9257e7c4-41c9-4c8f-9886-b035005e7945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f59a1118-ccb0-4485-88fa-7f99e31ba21c",
   "metadata": {},
   "source": [
    "# 1.Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85f9847d-0f9e-4b15-90a2-0a79ac53be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions loading data from json file to dataframe\n",
    "import json\n",
    "\n",
    "def load_json_data(domain1_train_file, domain2_train_file, test_data_file):\n",
    "    def load_json_lines(file):\n",
    "        with open(file, \"r\") as f:\n",
    "            return [json.loads(line) for line in f if line.strip()]  # avoid empty line\n",
    "\n",
    "    domain1_train_data = load_json_lines(domain1_train_file)\n",
    "    domain2_train_data = load_json_lines(domain2_train_file)\n",
    "    test_data = load_json_lines(test_data_file)\n",
    "\n",
    "    return domain1_train_data, domain2_train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to dataframe from list\n",
    "def convert_to_dataframe(domain1_train_data, domain2_train_data, test_data):\n",
    "\n",
    "    df_train_domain1 = pd.DataFrame(domain1_train_data)\n",
    "\n",
    "    df_train_domain2 = pd.DataFrame(domain2_train_data)\n",
    "\n",
    "    df_test = pd.DataFrame(test_data)\n",
    "\n",
    "    return df_train_domain1, df_train_domain2, df_test\n",
    "\n",
    "\n",
    "# Load data from json file to dataframe\n",
    "domain1_train_file = \"domain1_train_data.json\"\n",
    "domain2_train_file = \"domain2_train_data.json\"\n",
    "test_data_file = \"test_data.json\"\n",
    "\n",
    "domain1_train_data, domain2_train_data, test_data = load_json_data(domain1_train_file, domain2_train_file, test_data_file)\n",
    "df_train_domain1, df_train_domain2, df_test = convert_to_dataframe(domain1_train_data, domain2_train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cbf358-1176-4329-a7f9-92c6f688f81e",
   "metadata": {},
   "source": [
    "# 2.Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "198fa9c7-2520-4849-aa53-662c1cb03a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Undersampling to domain2\n",
    "df_major = df_train_domain2[df_train_domain2['label'] == 1]\n",
    "df_minor = df_train_domain2[df_train_domain2['label'] == 0]\n",
    "\n",
    "df_major_down = df_major.sample(len(df_minor), random_state=42)\n",
    "df_balanced = pd.concat([df_major_down, df_minor]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce35128c-21c2-4ef7-9ace-66dc9aee43ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text in training texts to same length\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, max_len=512, pad_token=0):\n",
    "        self.max_len = max_len\n",
    "        self.pad_token = pad_token\n",
    "        self.texts = [self._pad_or_truncate(seq) for seq in df['text']]\n",
    "        self.labels = df['label'].tolist() if 'label' in df.columns else None\n",
    "\n",
    "    def _pad_or_truncate(self, seq):\n",
    "        if len(seq) < self.max_len:\n",
    "            return seq + [self.pad_token] * (self.max_len - len(seq))\n",
    "        else:\n",
    "            return seq[:self.max_len]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_tensor = torch.tensor(self.texts[idx], dtype=torch.long)\n",
    "        if self.labels:\n",
    "            label_tensor = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "            return text_tensor, label_tensor\n",
    "        else:\n",
    "            return text_tensor\n",
    "\n",
    "# Create tow loaders for Transformer\n",
    "df_combined = pd.concat([df_train_domain1, df_balanced]).sample(frac=1).reset_index(drop=True)\n",
    "train_dataset = TextDataset(df_combined, max_len=512)\n",
    "test_dataset = TextDataset(df_test, max_len=512)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa8074-5d6c-4a81-a693-46642cf0d850",
   "metadata": {},
   "source": [
    "# 3.Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bc0c0d2-0c06-4053-aba2-9f16188dec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Transformer implementation\n",
    "'''\n",
    "vocab_size=17119+1\n",
    "emb_dim=128 -> dimensions for word token embeddings\n",
    "n_heads=4 -> lightweight with good performance\n",
    "n_layers=2 -> 2 layers of transformer encoders\n",
    "max_len=512 -> can cover 93.8% of the samples\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=17120, emb_dim=128, n_heads=4, n_layers=2, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.pos_encoding = self._positional_encoding(max_len, emb_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(emb_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def _positional_encoding(self, max_len, d_model):\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        i = torch.arange(0, d_model, 2)\n",
    "        angle_rates = 1 / torch.pow(10000, (i.float() / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos * angle_rates)\n",
    "        pe[:, 1::2] = torch.cos(pos * angle_rates)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        mask = (x == 0)\n",
    "        emb = self.embedding(x) + self.pos_encoding[:, :x.size(1)].to(x.device)\n",
    "        out = self.transformer(emb, src_key_padding_mask=mask)\n",
    "        out = out.mean(dim=1) \n",
    "        out = self.sigmoid(self.fc(out)).squeeze(-1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923da61-1109-469d-9077-966fd2ed22c2",
   "metadata": {},
   "source": [
    "# 4. Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "469246d7-8674-47a7-9da1-6bd0be416c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guanhanze/Anaconda/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 32.1812\n",
      "Epoch 2, Loss: 28.1049\n",
      "Epoch 3, Loss: 20.5170\n",
      "Epoch 4, Loss: 15.5824\n",
      "Epoch 5, Loss: 14.1194\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TransformerClassifier(vocab_size=17120).to(device)\n",
    "criterion = nn.BCELoss()  # already undersampling, no need for pos_weight\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # Output shape: [batch_size]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Update tqdm progress bar with current loss\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddec2ba8-5fba-43d1-b197-98713094d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in DataLoader(test_dataset, batch_size=32):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = (outputs > 0.5).long()\n",
    "        predictions.extend(preds.cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56c161a7-aa90-4555-9e6f-ead4272f10c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 2)\n",
      "   id  label\n",
      "0   0      1\n",
      "1   1      0\n",
      "2   2      0\n",
      "3   3      1\n",
      "4   4      0\n",
      "label\n",
      "1    2273\n",
      "0    1727\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Output prediction\n",
    "df_test['label'] = predictions\n",
    "\n",
    "df_submission = df_test[['id', 'label']]\n",
    "\n",
    "df_submission.to_csv(\"submission_trans_v1.csv\", index=False)\n",
    "\n",
    "print(df_submission.shape)          # (4000, 2)\n",
    "print(df_submission.head())        \n",
    "print(df_submission['label'].value_counts())  # check distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e6d60-2e63-486c-baa0-a695673c52b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
