{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580ab100-cd64-4111-858b-f87e0fe885cb",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd7ed5b-a209-41c7-ac99-6c26d4210303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324b253b-bf09-43cd-b2b2-f8931d496d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea98dd9b-bf51-42c4-a704-3704ce0aa1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, num_filters=100, filter_sizes=(3, 4, 5), dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)           # (batch_size, seq_len, emb_dim)\n",
    "        x = x.permute(0, 2, 1)          # (batch_size, emb_dim, seq_len)\n",
    "        convs = [F.relu(conv(x)) for conv in self.convs]  # list of (batch_size, num_filters, *)\n",
    "        pooled = [F.max_pool1d(c, kernel_size=c.size(2)).squeeze(2) for c in convs]\n",
    "        cat = torch.cat(pooled, dim=1)  # (batch_size, num_filters * len(filter_sizes))\n",
    "        dropped = self.dropout(cat)\n",
    "        logits = self.fc(dropped)\n",
    "        return logits.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2cd1a0-a73f-4fed-8e10-fd8e63ff2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(token_lists: List[List[int]]) -> List[str]:\n",
    "    return [\" \".join(map(str, tokens)) for tokens in token_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0f64e3-f5f6-4e3b-9ed0-7cac3d847a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_lines(path):\n",
    "    texts, labels = [], []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            texts.append(torch.tensor(ex['text'], dtype=torch.long))\n",
    "            labels.append(ex['label'])\n",
    "    return texts, labels\n",
    "\n",
    "def load_test_json(path):\n",
    "    texts = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            texts.append(torch.tensor(ex['text'], dtype=torch.long))\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75347502-119e-41a5-a260-d323637d2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embed_stats(token_lists: List[List[int]], embedding_matrix: np.ndarray) -> np.ndarray:\n",
    "    stats = []\n",
    "    for tokens in token_lists:\n",
    "        embedded = np.array([embedding_matrix[t] for t in tokens if t < len(embedding_matrix)])\n",
    "        if embedded.size == 0:\n",
    "            mean = np.zeros(embedding_matrix.shape[1])\n",
    "            std = np.zeros(embedding_matrix.shape[1])\n",
    "        else:\n",
    "            mean = embedded.mean(axis=0)\n",
    "            std = embedded.std(axis=0)\n",
    "        stats.append(np.concatenate([mean, std]))\n",
    "    return np.vstack(stats)\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4bc46f0-9590-4293-a902-5bd1cff585b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_sampler(domains, labels):\n",
    "    from collections import Counter\n",
    "    from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "    bucket_keys = list(zip(domains, labels))\n",
    "    bucket_counts = Counter(bucket_keys)\n",
    "    bucket_weights = {k: 1.0 / count for k, count in bucket_counts.items()}\n",
    "    sample_weights = torch.DoubleTensor([bucket_weights[k] for k in bucket_keys])\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae61815c-e2a6-4d66-981e-041788424687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_val_split(texts, labels, domains, val_size_per_group=60, random_state=42):\n",
    "    random.seed(random_state)\n",
    "    from collections import defaultdict\n",
    "    buckets = defaultdict(list)\n",
    "\n",
    "    for x, y, d in zip(texts, labels, domains):\n",
    "        buckets[(d, y)].append((x, y, d))\n",
    "\n",
    "    train, val = [], []\n",
    "    for key in buckets:\n",
    "        group = buckets[key]\n",
    "        random.shuffle(group)\n",
    "        n_val = min(val_size_per_group, len(group))\n",
    "        val.extend(group[:n_val])\n",
    "        train.extend(group[n_val:])\n",
    "\n",
    "    random.shuffle(train)\n",
    "    random.shuffle(val)\n",
    "    tx, ty, td = zip(*train)\n",
    "    vx, vy, vd = zip(*val)\n",
    "    return list(tx), list(ty), list(td), list(vx), list(vy), list(vd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "283937b9-54d6-4bc5-ac6b-33bbf821acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, lr, device=\"cpu\"):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    pos_weight = torch.tensor([\n",
    "        sum(1 for y in train_loader.dataset.tensors[1] if y == 0) /\n",
    "        sum(1 for y in train_loader.dataset.tensors[1] if y == 1)\n",
    "    ], device=device)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch} — Train Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a622eb6-7b55-47c8-a9ea-9a028979c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_confidence(model, dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    preds, probs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\", leave=False):\n",
    "            if len(batch) == 2:\n",
    "                X_batch, _ = batch  # Ignore labels\n",
    "            else:\n",
    "                X_batch = batch[0]\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            batch_probs = torch.sigmoid(logits).squeeze()\n",
    "\n",
    "            if batch_probs.ndim == 0:\n",
    "                batch_probs = batch_probs.unsqueeze(0)\n",
    "\n",
    "            batch_preds = (batch_probs >= 0.5).int().tolist()\n",
    "            preds.extend(batch_preds)\n",
    "            probs.extend(batch_probs.cpu().tolist())\n",
    "\n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37f29e3c-ebcb-466b-9223-6a2e9b667a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/q4lnp11x0dz2_w8bl88s6hwh0000gn/T/ipykernel_67370/539589169.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fold_train_padded = pad_sequence([torch.tensor(t) for t in fold_train_texts], batch_first=True, padding_value=0)\n",
      "/var/folders/k2/q4lnp11x0dz2_w8bl88s6hwh0000gn/T/ipykernel_67370/539589169.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fold_val_padded = pad_sequence([torch.tensor(t) for t in fold_val_texts], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0374\n",
      "[Meta Training] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.2077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.1035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/q4lnp11x0dz2_w8bl88s6hwh0000gn/T/ipykernel_67370/539589169.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fold_train_padded = pad_sequence([torch.tensor(t) for t in fold_train_texts], batch_first=True, padding_value=0)\n",
      "/var/folders/k2/q4lnp11x0dz2_w8bl88s6hwh0000gn/T/ipykernel_67370/539589169.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fold_val_padded = pad_sequence([torch.tensor(t) for t in fold_val_texts], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Training] Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/q4lnp11x0dz2_w8bl88s6hwh0000gn/T/ipykernel_67370/539589169.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fold_train_padded = pad_sequence([torch.tensor(t) for t in fold_train_texts], batch_first=True, padding_value=0)\n",
      "/var/folders/k2/q4lnp11x0dz2_w8bl88s6hwh0000gn/T/ipykernel_67370/539589169.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fold_val_padded = pad_sequence([torch.tensor(t) for t in fold_val_texts], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Training] Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/q4lnp11x0dz2_w8bl88s6hwh0000gn/T/ipykernel_67370/539589169.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fold_train_padded = pad_sequence([torch.tensor(t) for t in fold_train_texts], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Training] Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/q4lnp11x0dz2_w8bl88s6hwh0000gn/T/ipykernel_67370/539589169.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fold_val_padded = pad_sequence([torch.tensor(t) for t in fold_val_texts], batch_first=True, padding_value=0)\n",
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/q4lnp11x0dz2_w8bl88s6hwh0000gn/T/ipykernel_67370/539589169.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fold_train_padded = pad_sequence([torch.tensor(t) for t in fold_train_texts], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Training] Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/q4lnp11x0dz2_w8bl88s6hwh0000gn/T/ipykernel_67370/539589169.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fold_val_padded = pad_sequence([torch.tensor(t) for t in fold_val_texts], batch_first=True, padding_value=0)\n",
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    }
   ],
   "source": [
    "# === Step 0: Load and Split Data ===\n",
    "d1_texts, d1_labels = load_json_lines(\"data/domain1_train_data.json\")\n",
    "d2_texts, d2_labels = load_json_lines(\"data/domain2_train_data.json\")\n",
    "texts = d1_texts + d2_texts\n",
    "labels = d1_labels + d2_labels\n",
    "domains = [0] * len(d1_labels) + [1] * len(d2_labels)\n",
    "\n",
    "test_texts_raw = load_test_json(\"data/test_data.json\")\n",
    "test_texts = pad_sequence(test_texts_raw, batch_first=True, padding_value=0)\n",
    "\n",
    "test_dataset_cnn = TensorDataset(test_texts)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=32)\n",
    "\n",
    "# Stratified validation split: 60 from each domain\n",
    "train_texts, train_labels, train_domains, val_texts, val_labels, val_domains = stratified_train_val_split(\n",
    "    texts, labels, domains, val_size_per_group=60\n",
    ")\n",
    "\n",
    "# Pad after splitting\n",
    "train_texts_padded = pad_sequence(train_texts, batch_first=True, padding_value=0)\n",
    "val_texts_padded = pad_sequence(val_texts, batch_first=True, padding_value=0)\n",
    "\n",
    "# === Step 1: Token Conversion and Text Processing ===\n",
    "train_text_strings = tokens_to_text(train_texts)\n",
    "val_text_strings = tokens_to_text(val_texts)\n",
    "full_text_strings = tokens_to_text(train_texts + val_texts)\n",
    "test_text_strings = tokens_to_text(test_texts_raw)\n",
    "\n",
    "# === Step 2: TF-IDF + SVD for MLP ===\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_text_strings).toarray()\n",
    "X_val_tfidf = tfidf_vectorizer.transform(val_text_strings).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_text_strings).toarray()\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "X_val_svd = svd.transform(X_val_tfidf)\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "embedding_layer = nn.Embedding(17120, 128, padding_idx=0)\n",
    "embedding_matrix = embedding_layer.weight.detach().cpu().numpy()\n",
    "\n",
    "train_embed_stats = compute_embed_stats(train_texts, embedding_matrix)\n",
    "val_embed_stats = compute_embed_stats(val_texts, embedding_matrix)\n",
    "test_embed_stats = compute_embed_stats(test_texts_raw, embedding_matrix)\n",
    "\n",
    "X_train_raw = np.hstack([X_train_svd, train_embed_stats])\n",
    "X_val_raw = np.hstack([X_val_svd, val_embed_stats])\n",
    "X_test_raw = np.hstack([X_test_svd, test_embed_stats])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train_raw)\n",
    "X_val_std = scaler.transform(X_val_raw)\n",
    "X_test_std = scaler.transform(X_test_raw)\n",
    "\n",
    "# === Step 3: Prepare MLP Datasets ===\n",
    "X_train_tensor = torch.tensor(X_train_std, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_std, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(val_labels, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_std, dtype=torch.float32)\n",
    "\n",
    "train_dataset_mlp = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset_mlp = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset_mlp = TensorDataset(X_test_tensor)\n",
    "\n",
    "train_loader_mlp = DataLoader(train_dataset_mlp, batch_size=64, sampler=create_weighted_sampler(train_domains, train_labels))\n",
    "val_loader_mlp = DataLoader(val_dataset_mlp, batch_size=64)\n",
    "test_loader_mlp = DataLoader(test_dataset_mlp, batch_size=64)\n",
    "\n",
    "# === Step 4: Train MLP on Train Set ===\n",
    "mlp_model = MLPClassifier(input_dim=X_train_tensor.shape[1], hidden_dim=256, dropout=0.5).to(device)\n",
    "train(mlp_model, train_loader_mlp, epochs=6, lr=1e-3, device=device)\n",
    "\n",
    "# === Step 5: Train CNN on Train Set ===\n",
    "\n",
    "\n",
    "train_dataset_cnn = TensorDataset(train_texts_padded, torch.tensor(train_labels, dtype=torch.float32))\n",
    "val_dataset_cnn = TensorDataset(val_texts_padded, torch.tensor(val_labels, dtype=torch.float32))\n",
    "test_dataset_cnn = TensorDataset(test_texts)\n",
    "\n",
    "train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=32, sampler=create_weighted_sampler(train_domains, train_labels))\n",
    "val_loader_cnn = DataLoader(val_dataset_cnn, batch_size=32)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=32)\n",
    "\n",
    "cnn_model = CNNTextClassifier(\n",
    "    vocab_size=17120,\n",
    "    embedding_dim=128,\n",
    "    num_filters=50,\n",
    "    filter_sizes=(2, 3, 4),\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "train(cnn_model, train_loader_cnn, epochs=4, lr=0.001, device=device)\n",
    "\n",
    "# === Step 6: Generate Out-of-Fold Predictions for Meta-Classifier ===\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "meta_inputs = []\n",
    "meta_targets = []\n",
    "cnn_val_preds_all = []\n",
    "mlp_val_preds_all = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
    "    print(f\"[Meta Training] Fold {fold + 1}/{num_folds}\")\n",
    "    fold_train_texts = [texts[i] for i in train_idx]\n",
    "    fold_val_texts = [texts[i] for i in val_idx]\n",
    "    fold_train_labels = [labels[i] for i in train_idx]\n",
    "    fold_val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "    # Prepare padded CNN input\n",
    "    fold_train_padded = pad_sequence([torch.tensor(t) for t in fold_train_texts], batch_first=True, padding_value=0)\n",
    "    fold_val_padded = pad_sequence([torch.tensor(t) for t in fold_val_texts], batch_first=True, padding_value=0)\n",
    "\n",
    "    # CNN\n",
    "    cnn_model = CNNTextClassifier(\n",
    "        vocab_size=17120,\n",
    "        embedding_dim=128,\n",
    "        num_filters=50,\n",
    "        filter_sizes=(2, 3, 4),\n",
    "        dropout=0.5\n",
    "    ).to(device)\n",
    "    fold_train_domains = [domains[i] for i in train_idx]\n",
    "    cnn_loader = DataLoader(\n",
    "        TensorDataset(fold_train_padded, torch.tensor(fold_train_labels, dtype=torch.float32)),\n",
    "        batch_size=32,\n",
    "        sampler=create_weighted_sampler(fold_train_domains, fold_train_labels)\n",
    "    )\n",
    "    train(cnn_model, cnn_loader, epochs=4, lr=0.001, device=device)\n",
    "\n",
    "    val_loader_cnn = DataLoader(TensorDataset(fold_val_padded), batch_size=32)\n",
    "    cnn_probs = np.array(predict_with_confidence(cnn_model, val_loader_cnn, device=device)[1])\n",
    "\n",
    "    # MLP features\n",
    "    fold_train_strings = tokens_to_text(fold_train_texts)\n",
    "    fold_val_strings = tokens_to_text(fold_val_texts)\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = tfidf.fit_transform(fold_train_strings).toarray()\n",
    "    X_val_tfidf = tfidf.transform(fold_val_strings).toarray()\n",
    "\n",
    "    svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "    X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "    X_val_svd = svd.transform(X_val_tfidf)\n",
    "\n",
    "    embed_stats_train = compute_embed_stats(fold_train_texts, embedding_matrix)\n",
    "    embed_stats_val = compute_embed_stats(fold_val_texts, embedding_matrix)\n",
    "\n",
    "    X_train_raw = np.hstack([X_train_svd, embed_stats_train])\n",
    "    X_val_raw = np.hstack([X_val_svd, embed_stats_val])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(X_train_raw)\n",
    "    X_val_std = scaler.transform(X_val_raw)\n",
    "\n",
    "    mlp_model = MLPClassifier(input_dim=X_train_std.shape[1], hidden_dim=256, dropout=0.5).to(device)\n",
    "    train_loader_mlp = DataLoader(\n",
    "        TensorDataset(torch.tensor(X_train_std, dtype=torch.float32), torch.tensor(fold_train_labels, dtype=torch.float32)),\n",
    "        batch_size=64,\n",
    "        sampler=create_weighted_sampler(fold_train_domains, fold_train_labels)\n",
    "    )\n",
    "    train(mlp_model, train_loader_mlp, epochs=6, lr=1e-3, device=device)\n",
    "\n",
    "    val_loader_mlp = DataLoader(TensorDataset(torch.tensor(X_val_std, dtype=torch.float32)), batch_size=64)\n",
    "    mlp_probs = np.array(predict_with_confidence(mlp_model, val_loader_mlp, device=device)[1])\n",
    "\n",
    "    fold_meta = np.vstack([\n",
    "        cnn_probs,\n",
    "        (cnn_probs >= 0.5).astype(float),\n",
    "        np.abs(cnn_probs - 0.5),\n",
    "        mlp_probs,\n",
    "        (mlp_probs >= 0.5).astype(float),\n",
    "        np.abs(mlp_probs - 0.5)\n",
    "    ]).T\n",
    "    cnn_val_preds_all.append((cnn_probs >= 0.5).astype(int))\n",
    "    mlp_val_preds_all.append((mlp_probs >= 0.5).astype(int))\n",
    "    meta_inputs.append(fold_meta)\n",
    "    meta_targets.extend(fold_val_labels)\n",
    "\n",
    "X_meta_val = np.vstack(meta_inputs)\n",
    "y_meta_val = np.array(meta_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67045784-dc3c-4557-9f8b-c03e3a286908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MetaNN] Epoch 1 — Loss: 0.1448\n",
      "[MetaNN] Epoch 2 — Loss: 0.1069\n",
      "[MetaNN] Epoch 3 — Loss: 0.0812\n",
      "[MetaNN] Epoch 4 — Loss: 0.0707\n",
      "[MetaNN] Epoch 5 — Loss: 0.0662\n",
      "[MetaNN] Epoch 6 — Loss: 0.0651\n",
      "[MetaNN] Epoch 7 — Loss: 0.0655\n",
      "[MetaNN] Epoch 8 — Loss: 0.0643\n",
      "[MetaNN] Epoch 9 — Loss: 0.0638\n",
      "[MetaNN] Epoch 10 — Loss: 0.0643\n",
      "MetaNN validation accuracy: 0.9192\n",
      "CNN val acc: 0.87\n",
      "MLP val acc: 0.8931666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final ensemble predictions saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# === Step 6b: Train Meta-NN Classifier and Evaluate on Validation ===\n",
    "\n",
    "class MetaNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(6, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "meta_nn = MetaNN().to(device)\n",
    "optimizer_meta = torch.optim.Adam(meta_nn.parameters(), lr=1e-3)\n",
    "pos_weight = torch.tensor([\n",
    "    (y_meta_val_tensor == 0).sum() / (y_meta_val_tensor == 1).sum()\n",
    "], device=device)\n",
    "criterion_meta = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "X_meta_val_tensor = torch.tensor(X_meta_val, dtype=torch.float32).to(device)\n",
    "y_meta_val_tensor = torch.tensor(y_meta_val, dtype=torch.float32).to(device)\n",
    "\n",
    "meta_dataset = TensorDataset(X_meta_val_tensor, y_meta_val_tensor)\n",
    "meta_loader = DataLoader(meta_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    total_loss = 0.0\n",
    "    meta_nn.train()\n",
    "    for X_batch, y_batch in meta_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer_meta.zero_grad()\n",
    "        logits = meta_nn(X_batch)\n",
    "        loss = criterion_meta(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_meta.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[MetaNN] Epoch {epoch} — Loss: {total_loss / len(meta_loader):.4f}\")\n",
    "\n",
    "# === MetaNN Validation Accuracy ===\n",
    "meta_nn.eval()\n",
    "with torch.no_grad():\n",
    "    val_logits = meta_nn(X_meta_val_tensor)\n",
    "    val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
    "    val_preds = (val_probs >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "val_acc = accuracy_score(y_meta_val, val_preds)\n",
    "print(f\"MetaNN validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "cnn_val_preds = np.concatenate(cnn_val_preds_all)\n",
    "mlp_val_preds = np.concatenate(mlp_val_preds_all)\n",
    "print(\"CNN val acc:\", accuracy_score(y_meta_val, cnn_val_preds))\n",
    "print(\"MLP val acc:\", accuracy_score(y_meta_val, mlp_val_preds))\n",
    "\n",
    "# === Step 7: Recompute Features and Retrain CNN and MLP on Full Set ===\n",
    "# Recreate datasets first\n",
    "full_texts_raw = train_texts + val_texts\n",
    "full_labels = train_labels + val_labels\n",
    "full_text_strings = tokens_to_text(full_texts_raw)\n",
    "\n",
    "# Refit TF-IDF and SVD\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_full_tfidf = tfidf_vectorizer.fit_transform(full_text_strings).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_text_strings).toarray()\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_full_svd = svd.fit_transform(X_full_tfidf)\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "full_embed_stats = compute_embed_stats(full_texts_raw, embedding_matrix)\n",
    "test_embed_stats = compute_embed_stats(test_texts, embedding_matrix)\n",
    "\n",
    "X_full_raw = np.hstack([X_full_svd, full_embed_stats])\n",
    "X_test_raw = np.hstack([X_test_svd, test_embed_stats])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_full_std = scaler.fit_transform(X_full_raw)\n",
    "X_test_std = scaler.transform(X_test_raw)\n",
    "\n",
    "X_full_tensor = torch.tensor(X_full_std, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_std, dtype=torch.float32)\n",
    "# Recreate padded dataset for CNN\n",
    "full_texts_padded = pad_sequence(train_texts + val_texts, batch_first=True, padding_value=0)\n",
    "full_labels = train_labels + val_labels\n",
    "\n",
    "# X_full_tensor already set above with recomputed features\n",
    "y_full_tensor = torch.tensor(full_labels, dtype=torch.float32)\n",
    "full_dataset_mlp = TensorDataset(X_full_tensor, y_full_tensor)\n",
    "full_loader_mlp = DataLoader(full_dataset_mlp, batch_size=64, sampler=create_weighted_sampler(train_domains + val_domains, full_labels))\n",
    "\n",
    "mlp_model = MLPClassifier(input_dim=X_full_tensor.shape[1], hidden_dim=256, dropout=0.5).to(device)\n",
    "train(mlp_model, full_loader_mlp, epochs=6, lr=1e-3, device=device)\n",
    "\n",
    "\n",
    "full_dataset_cnn = TensorDataset(full_texts_padded, torch.tensor(full_labels, dtype=torch.float32))\n",
    "full_loader_cnn = DataLoader(full_dataset_cnn, batch_size=32, sampler=create_weighted_sampler(train_domains + val_domains, full_labels))\n",
    "\n",
    "cnn_model = CNNTextClassifier(\n",
    "    vocab_size=17120,\n",
    "    embedding_dim=128,\n",
    "    num_filters=50,\n",
    "    filter_sizes=(2, 3, 4),\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "train(cnn_model, full_loader_cnn, epochs=4, lr=0.001, device=device)\n",
    "\n",
    "# === Step 8: Predict on Test and Use Meta-Classifier ===\n",
    "mlp_test_preds, mlp_test_probs = predict_with_confidence(mlp_model, test_loader_mlp, device=device)\n",
    "cnn_test_preds, cnn_test_probs = predict_with_confidence(cnn_model, test_loader_cnn, device=device)\n",
    "\n",
    "cnn_test_probs = np.array(cnn_test_probs)\n",
    "mlp_test_probs = np.array(mlp_test_probs)\n",
    "X_meta_test = np.vstack([\n",
    "    cnn_test_probs,\n",
    "    (cnn_test_probs >= 0.5).astype(float),\n",
    "    np.abs(cnn_test_probs - 0.5),\n",
    "    mlp_test_probs,\n",
    "    (mlp_test_probs >= 0.5).astype(float),\n",
    "    np.abs(mlp_test_probs - 0.5)\n",
    "]).T\n",
    "meta_nn.eval()\n",
    "X_meta_test_tensor = torch.tensor(X_meta_test, dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = meta_nn(X_meta_test_tensor)\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "ensemble_probs = probs\n",
    "ensemble_preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "# === Step 9: Save Predictions ===\n",
    "results_df = pd.DataFrame({\n",
    "    \"id\": list(range(len(ensemble_preds))),\n",
    "    \"label\": ensemble_preds.astype(int),\n",
    "    \"confidence\": ensemble_probs\n",
    "})\n",
    "results_df.to_csv(\"ensemble_submission_final.csv\", index=False)\n",
    "results_df[[\"id\", \"label\"]].to_csv(\"ensemble_labels_final.csv\", index=False)\n",
    "print(\"Final ensemble predictions saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
