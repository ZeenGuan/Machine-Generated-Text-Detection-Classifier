{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580ab100-cd64-4111-858b-f87e0fe885cb",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd7ed5b-a209-41c7-ac99-6c26d4210303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324b253b-bf09-43cd-b2b2-f8931d496d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea98dd9b-bf51-42c4-a704-3704ce0aa1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, num_filters=100, filter_sizes=(3, 4, 5), dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)           # (batch_size, seq_len, emb_dim)\n",
    "        x = x.permute(0, 2, 1)          # (batch_size, emb_dim, seq_len)\n",
    "        convs = [F.relu(conv(x)) for conv in self.convs]  # list of (batch_size, num_filters, *)\n",
    "        pooled = [F.max_pool1d(c, kernel_size=c.size(2)).squeeze(2) for c in convs]\n",
    "        cat = torch.cat(pooled, dim=1)  # (batch_size, num_filters * len(filter_sizes))\n",
    "        dropped = self.dropout(cat)\n",
    "        logits = self.fc(dropped)\n",
    "        return logits.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b38f4c3-70dd-4566-9fd1-d6d5836432a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(embeds)\n",
    "        forward_final = h_n[-2]\n",
    "        backward_final = h_n[-1]\n",
    "        last_hidden = torch.cat((forward_final, backward_final), dim=1)\n",
    "        logits = self.fc(self.dropout(last_hidden))\n",
    "        return logits.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2cd1a0-a73f-4fed-8e10-fd8e63ff2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(token_lists: List[List[int]]) -> List[str]:\n",
    "    return [\" \".join(map(str, tokens)) for tokens in token_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0f64e3-f5f6-4e3b-9ed0-7cac3d847a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_lines(path):\n",
    "    texts, labels = [], []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            texts.append(torch.tensor(ex['text'], dtype=torch.long))\n",
    "            labels.append(ex['label'])\n",
    "    return texts, labels\n",
    "\n",
    "def load_test_json(path):\n",
    "    texts = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            texts.append(torch.tensor(ex['text'], dtype=torch.long))\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75347502-119e-41a5-a260-d323637d2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embed_stats(token_lists: List[List[int]], embedding_matrix: np.ndarray) -> np.ndarray:\n",
    "    stats = []\n",
    "    for tokens in token_lists:\n",
    "        embedded = np.array([embedding_matrix[t] for t in tokens if t < len(embedding_matrix)])\n",
    "        if embedded.size == 0:\n",
    "            mean = np.zeros(embedding_matrix.shape[1])\n",
    "            std = np.zeros(embedding_matrix.shape[1])\n",
    "        else:\n",
    "            mean = embedded.mean(axis=0)\n",
    "            std = embedded.std(axis=0)\n",
    "        stats.append(np.concatenate([mean, std]))\n",
    "    return np.vstack(stats)\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4bc46f0-9590-4293-a902-5bd1cff585b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_sampler(domains, labels):\n",
    "    from collections import defaultdict\n",
    "    from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "    # Group by (domain, label)\n",
    "    group_to_indices = defaultdict(list)\n",
    "    for i, (d, l) in enumerate(zip(domains, labels)):\n",
    "        group_to_indices[(d, l)].append(i)\n",
    "\n",
    "    # Find the max group size\n",
    "    max_group_size = max(len(v) for v in group_to_indices.values())\n",
    "\n",
    "    # Create weights so each group contributes equally\n",
    "    weights = [0] * len(labels)\n",
    "    for group, indices in group_to_indices.items():\n",
    "        group_weight = max_group_size / len(indices)\n",
    "        for i in indices:\n",
    "            weights[i] = group_weight\n",
    "\n",
    "    sample_weights = torch.DoubleTensor(weights)\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(labels), replacement=True)\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae61815c-e2a6-4d66-981e-041788424687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_val_split(texts, labels, domains, val_size_per_group=60, random_state=42):\n",
    "    random.seed(random_state)\n",
    "    from collections import defaultdict\n",
    "    buckets = defaultdict(list)\n",
    "\n",
    "    for x, y, d in zip(texts, labels, domains):\n",
    "        buckets[(d, y)].append((x, y, d))\n",
    "\n",
    "    train, val = [], []\n",
    "    for key in buckets:\n",
    "        group = buckets[key]\n",
    "        random.shuffle(group)\n",
    "        n_val = min(val_size_per_group, len(group))\n",
    "        val.extend(group[:n_val])\n",
    "        train.extend(group[n_val:])\n",
    "\n",
    "    random.shuffle(train)\n",
    "    random.shuffle(val)\n",
    "    tx, ty, td = zip(*train)\n",
    "    vx, vy, vd = zip(*val)\n",
    "    return list(tx), list(ty), list(td), list(vx), list(vy), list(vd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bdd5a582-3896-42de-8ef3-82ac3e03beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_kfold_by_domain_label(texts, labels, domains, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Custom stratified K-Fold by (domain, label) pair.\n",
    "    Returns list of (train_indices, val_indices) for each fold.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    buckets = defaultdict(list)\n",
    "\n",
    "    for idx, (d, l) in enumerate(zip(domains, labels)):\n",
    "        buckets[(d, l)].append(idx)\n",
    "\n",
    "    for key in buckets:\n",
    "        rng.shuffle(buckets[key])\n",
    "\n",
    "    folds = [[] for _ in range(n_splits)]\n",
    "    for key, idxs in buckets.items():\n",
    "        for i, idx in enumerate(idxs):\n",
    "            folds[i % n_splits].append(idx)\n",
    "\n",
    "    fold_indices = []\n",
    "    for i in range(n_splits):\n",
    "        val_idx = folds[i]\n",
    "        train_idx = [idx for j in range(n_splits) if j != i for idx in folds[j]]\n",
    "        fold_indices.append((train_idx, val_idx))\n",
    "\n",
    "    return fold_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "283937b9-54d6-4bc5-ac6b-33bbf821acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, lr, device=\"cpu\"):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    pos_weight = torch.tensor([\n",
    "        sum(1 for y in train_loader.dataset.tensors[1] if y == 0) /\n",
    "        sum(1 for y in train_loader.dataset.tensors[1] if y == 1)\n",
    "    ], device=device)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch} — Train Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a622eb6-7b55-47c8-a9ea-9a028979c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_confidence(model, dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    preds, probs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\", leave=False):\n",
    "            if len(batch) == 2:\n",
    "                X_batch, _ = batch  # Ignore labels\n",
    "            else:\n",
    "                X_batch = batch[0]\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            batch_probs = torch.sigmoid(logits).squeeze()\n",
    "\n",
    "            if batch_probs.ndim == 0:\n",
    "                batch_probs = batch_probs.unsqueeze(0)\n",
    "\n",
    "            batch_preds = (batch_probs >= 0.5).int().tolist()\n",
    "            preds.extend(batch_preds)\n",
    "            probs.extend(batch_probs.cpu().tolist())\n",
    "\n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "71f87874-7077-46d3-96c9-3c1c63edd0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Training] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.2078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.1088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.1153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Training] Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.2060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.1086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Training] Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.2116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.1091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Training] Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.2120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.1064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.1142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta Training] Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.1061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    }
   ],
   "source": [
    "# === Step 0: Load and Split Data ===\n",
    "d1_texts, d1_labels = load_json_lines(\"data/domain1_train_data.json\")\n",
    "d2_texts, d2_labels = load_json_lines(\"data/domain2_train_data.json\")\n",
    "texts = d1_texts + d2_texts\n",
    "labels = d1_labels + d2_labels\n",
    "domains = [0] * len(d1_labels) + [1] * len(d2_labels)\n",
    "\n",
    "test_texts_raw = load_test_json(\"data/test_data.json\")\n",
    "test_texts = pad_sequence(test_texts_raw, batch_first=True, padding_value=0)\n",
    "\n",
    "test_dataset_cnn = TensorDataset(test_texts)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=32)\n",
    "\n",
    "# === Step 6: Generate Out-of-Fold Predictions for Meta-Classifier ===\n",
    "meta_inputs = []\n",
    "meta_targets = []\n",
    "bilstm_val_preds_all = []\n",
    "cnn_val_preds_all = []\n",
    "mlp_val_preds_all = []\n",
    "\n",
    "fold_indices = stratified_kfold_by_domain_label(texts, labels, domains, n_splits=5)\n",
    "for fold, (train_idx, val_idx) in enumerate(fold_indices):\n",
    "    print(f\"[Meta Training] Fold {fold + 1}/{num_folds}\")\n",
    "    fold_train_texts = [texts[i] for i in train_idx]\n",
    "    fold_val_texts = [texts[i] for i in val_idx]\n",
    "    fold_train_labels = [labels[i] for i in train_idx]\n",
    "    fold_val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "    # Prepare padded CNN input\n",
    "    fold_train_padded = pad_sequence([t.clone().detach() if isinstance(t, torch.Tensor) else torch.tensor(t).clone().detach() for t in fold_train_texts], batch_first=True, padding_value=0)\n",
    "    fold_val_padded = pad_sequence([t.clone().detach() if isinstance(t, torch.Tensor) else torch.tensor(t).clone().detach() for t in fold_val_texts], batch_first=True, padding_value=0)\n",
    "\n",
    "    # CNN\n",
    "    cnn_model = CNNTextClassifier(\n",
    "        vocab_size=17120,\n",
    "        embedding_dim=128,\n",
    "        num_filters=50,\n",
    "        filter_sizes=(2, 3, 4),\n",
    "        dropout=0.5\n",
    "    ).to(device)\n",
    "    fold_train_domains = [domains[i] for i in train_idx]\n",
    "    cnn_loader = DataLoader(\n",
    "        TensorDataset(fold_train_padded, torch.tensor(fold_train_labels, dtype=torch.float32)),\n",
    "        batch_size=32,\n",
    "        sampler=create_weighted_sampler(fold_train_domains, fold_train_labels)\n",
    "    )\n",
    "    train(cnn_model, cnn_loader, epochs=4, lr=0.001, device=device)\n",
    "\n",
    "    val_loader_cnn = DataLoader(TensorDataset(fold_val_padded), batch_size=32)\n",
    "    cnn_probs = np.array(predict_with_confidence(cnn_model, val_loader_cnn, device=device)[1])\n",
    "\n",
    "    # BiLSTM\n",
    "    bilstm_model = BiLSTMClassifier(\n",
    "        vocab_size=17120,\n",
    "        hidden_dim=128,\n",
    "        num_layers=1,\n",
    "        dropout=0.5\n",
    "    ).to(device)\n",
    "    bilstm_loader = DataLoader(\n",
    "        TensorDataset(fold_train_padded, torch.tensor(fold_train_labels, dtype=torch.float32)),\n",
    "        batch_size=32,\n",
    "        sampler=create_weighted_sampler(fold_train_domains, fold_train_labels)\n",
    "    )\n",
    "    train(bilstm_model, bilstm_loader, epochs=5, lr=0.0005, device=device)\n",
    "\n",
    "    val_loader_bilstm = DataLoader(TensorDataset(fold_val_padded), batch_size=32)\n",
    "    bilstm_probs = np.array(predict_with_confidence(bilstm_model, val_loader_bilstm, device=device)[1])\n",
    "    \n",
    "    #MLP\n",
    "    fold_val_strings = tokens_to_text(fold_val_texts)\n",
    "    fold_train_strings = tokens_to_text(fold_train_texts)\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = tfidf.fit_transform(fold_train_strings).toarray()\n",
    "    X_val_tfidf = tfidf.transform(fold_val_strings).toarray()\n",
    "\n",
    "    svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "    X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "    X_val_svd = svd.transform(X_val_tfidf)\n",
    "\n",
    "    X_train_raw = np.hstack([X_train_svd])\n",
    "    X_val_raw = np.hstack([X_val_svd])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(X_train_raw)\n",
    "    X_val_std = scaler.transform(X_val_raw)\n",
    "\n",
    "    mlp_model = MLPClassifier(input_dim=X_train_std.shape[1], hidden_dim=256, dropout=0.5).to(device)\n",
    "    train_loader_mlp = DataLoader(\n",
    "        TensorDataset(torch.tensor(X_train_std, dtype=torch.float32), torch.tensor(fold_train_labels, dtype=torch.float32)),\n",
    "        batch_size=64,\n",
    "        sampler=create_weighted_sampler(fold_train_domains, fold_train_labels)\n",
    "    )\n",
    "    train(mlp_model, train_loader_mlp, epochs=6, lr=1e-3, device=device)\n",
    "\n",
    "    val_loader_mlp = DataLoader(TensorDataset(torch.tensor(X_val_std, dtype=torch.float32)), batch_size=64)\n",
    "    mlp_probs = np.array(predict_with_confidence(mlp_model, val_loader_mlp, device=device)[1])\n",
    "\n",
    "    fold_meta = np.vstack([\n",
    "        cnn_probs,\n",
    "        (cnn_probs >= 0.5).astype(float),\n",
    "        np.abs(cnn_probs - 0.5),\n",
    "        mlp_probs,\n",
    "        (mlp_probs >= 0.5).astype(float),\n",
    "        np.abs(mlp_probs - 0.5),\n",
    "        bilstm_probs,\n",
    "        (bilstm_probs >= 0.5).astype(float),\n",
    "        np.abs(bilstm_probs - 0.5)\n",
    "    ]).T\n",
    "    bilstm_val_preds_all.append((bilstm_probs >= 0.5).astype(int))\n",
    "    cnn_val_preds_all.append((cnn_probs >= 0.5).astype(int))\n",
    "    mlp_val_preds_all.append((mlp_probs >= 0.5).astype(int))\n",
    "    meta_inputs.append(fold_meta)\n",
    "    meta_targets.extend(fold_val_labels)\n",
    "\n",
    "X_meta_val = np.vstack(meta_inputs)\n",
    "y_meta_val = np.array(meta_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de88f1fd-dd4c-4d27-881b-d7131b6d78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(9, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf5186f-7a35-47b3-9298-557506238c72",
   "metadata": {},
   "source": [
    "Found that meta-classifier has best performance training for 6 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cc171473-ee36-45ac-a52b-2ce8272d48e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MetaNN] Epoch 1 — Train Loss: 0.1178 — Val Acc: 0.8083\n",
      "[MetaNN] Epoch 2 — Train Loss: 0.0935 — Val Acc: 0.8438\n",
      "[MetaNN] Epoch 3 — Train Loss: 0.0729 — Val Acc: 0.9083\n",
      "[MetaNN] Epoch 4 — Train Loss: 0.0605 — Val Acc: 0.9042\n",
      "[MetaNN] Epoch 5 — Train Loss: 0.0518 — Val Acc: 0.9062\n",
      "[MetaNN] Epoch 6 — Train Loss: 0.0503 — Val Acc: 0.9062\n",
      "[MetaNN] Epoch 7 — Train Loss: 0.0479 — Val Acc: 0.9042\n",
      "[MetaNN] Epoch 8 — Train Loss: 0.0459 — Val Acc: 0.9042\n",
      "Early stopping.\n",
      "\n",
      "MetaNN validation accuracy (on full out-of-fold set): 0.9183\n",
      "CNN val acc: 0.8716666666666667\n",
      "MLP val acc: 0.8096666666666666\n",
      "BiLSTM val acc: 0.8715\n"
     ]
    }
   ],
   "source": [
    "# === Step 7a: Train and Validate MetaNN ===\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create stratified split for meta-training\n",
    "meta_domains = list(np.array(domains)[np.concatenate([val_idx for _, val_idx in skf.split(texts, labels)])])\n",
    "meta_texts = list(X_meta_val)\n",
    "meta_labels = list(y_meta_val)\n",
    "\n",
    "meta_train_texts, meta_train_labels, _, meta_val_texts, meta_val_labels, _ = stratified_train_val_split(\n",
    "    meta_texts, meta_labels, meta_domains, val_size_per_group=120, random_state=42\n",
    ")\n",
    "\n",
    "X_meta_train_tensor = torch.tensor(meta_train_texts, dtype=torch.float32).to(device)\n",
    "y_meta_train_tensor = torch.tensor(meta_train_labels, dtype=torch.float32).to(device)\n",
    "X_meta_valid_tensor = torch.tensor(meta_val_texts, dtype=torch.float32).to(device)\n",
    "y_meta_valid_tensor = torch.tensor(meta_val_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define MetaNN\n",
    "meta_nn = MetaNN().to(device)\n",
    "optimizer_meta = torch.optim.Adam(meta_nn.parameters(), lr=1e-3)\n",
    "pos_weight = torch.tensor([(y_meta_train_tensor == 0).sum() / (y_meta_train_tensor == 1).sum()], device=device)\n",
    "criterion_meta = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "meta_loader = DataLoader(TensorDataset(X_meta_train_tensor, y_meta_train_tensor), batch_size=64, shuffle=True)\n",
    "\n",
    "# Early stopping\n",
    "best_val_acc = 0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    meta_nn.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in meta_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer_meta.zero_grad()\n",
    "        logits = meta_nn(X_batch)\n",
    "        loss = criterion_meta(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_meta.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    meta_nn.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = meta_nn(X_meta_valid_tensor)\n",
    "        val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
    "        val_preds = (val_probs >= 0.5).astype(int)\n",
    "        val_acc = accuracy_score(y_meta_valid_tensor.cpu().numpy(), val_preds)\n",
    "\n",
    "    print(f\"[MetaNN] Epoch {epoch} — Train Loss: {total_loss / len(meta_loader):.4f} — Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = meta_nn.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "meta_nn.load_state_dict(best_state)\n",
    "\n",
    "# Evaluate MetaNN against full validation meta set\n",
    "meta_nn.eval()\n",
    "with torch.no_grad():\n",
    "    val_logits = meta_nn(torch.tensor(X_meta_val, dtype=torch.float32).to(device))\n",
    "    val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
    "    val_preds = (val_probs >= 0.5).astype(int)\n",
    "\n",
    "meta_val_acc = accuracy_score(y_meta_val, val_preds)\n",
    "print(f\"\\nMetaNN validation accuracy (on full out-of-fold set): {meta_val_acc:.4f}\")\n",
    "\n",
    "cnn_val_preds = np.concatenate(cnn_val_preds_all)\n",
    "mlp_val_preds = np.concatenate(mlp_val_preds_all)\n",
    "bilstm_val_preds = np.concatenate(bilstm_val_preds_all)\n",
    "\n",
    "print(\"CNN val acc:\", accuracy_score(y_meta_val, cnn_val_preds))\n",
    "print(\"MLP val acc:\", accuracy_score(y_meta_val, mlp_val_preds))\n",
    "print(\"BiLSTM val acc:\", accuracy_score(y_meta_val, bilstm_val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "221e1c40-723c-4256-8fa9-025e604ac855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MetaNN] Epoch 1 — Train Loss: 0.1659\n",
      "[MetaNN] Epoch 2 — Train Loss: 0.1310\n",
      "[MetaNN] Epoch 3 — Train Loss: 0.0928\n",
      "[MetaNN] Epoch 4 — Train Loss: 0.0705\n",
      "[MetaNN] Epoch 5 — Train Loss: 0.0649\n",
      "[MetaNN] Epoch 6 — Train Loss: 0.0637\n",
      "[MetaNN] Epoch 7 — Train Loss: 0.0623\n",
      "[MetaNN] Epoch 8 — Train Loss: 0.0636\n",
      "[MetaNN] Epoch 9 — Train Loss: 0.0609\n",
      "[MetaNN] Epoch 10 — Train Loss: 0.0612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/q4lnp11x0dz2_w8bl88s6hwh0000gn/T/ipykernel_67370/3244026936.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  full_texts_padded = pad_sequence([torch.tensor(t) for t in full_texts], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train Loss: 0.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.2031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.1079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k2/q4lnp11x0dz2_w8bl88s6hwh0000gn/T/ipykernel_67370/3244026936.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  full_texts_padded = pad_sequence([torch.tensor(t) for t in full_texts], batch_first=True, padding_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — Train Loss: 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train Loss: 0.1587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — Train Loss: 0.0878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — Train Loss: 0.0572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — Train Loss: 0.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ensemble predictions saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# === Step 7: Retrain CNN and MLP on Full Data for Final Ensemble ===\n",
    "\n",
    "# === Train MetaNN on Full Meta-Training Set ===\n",
    "meta_nn = MetaNN().to(device)\n",
    "optimizer_meta = torch.optim.Adam(meta_nn.parameters(), lr=1e-3)\n",
    "\n",
    "pos_weight = torch.tensor([\n",
    "    (y_meta_val == 0).sum() / (y_meta_val == 1).sum()\n",
    "], device=device)\n",
    "criterion_meta = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "X_meta_val_tensor = torch.tensor(X_meta_val, dtype=torch.float32).to(device)\n",
    "y_meta_val_tensor = torch.tensor(y_meta_val, dtype=torch.float32).to(device)\n",
    "\n",
    "meta_loader = DataLoader(TensorDataset(X_meta_val_tensor, y_meta_val_tensor), batch_size=64, shuffle=True)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    total_loss = 0.0\n",
    "    meta_nn.train()\n",
    "    for X_batch, y_batch in meta_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer_meta.zero_grad()\n",
    "        logits = meta_nn(X_batch)\n",
    "        loss = criterion_meta(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_meta.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[MetaNN] Epoch {epoch} — Train Loss: {total_loss / len(meta_loader):.4f}\")\n",
    "full_texts = train_texts + val_texts\n",
    "full_labels = train_labels + val_labels\n",
    "full_domains = train_domains + val_domains\n",
    "\n",
    "# === Full MLP Training ===\n",
    "full_text_strings = tokens_to_text(full_texts)\n",
    "X_full_tfidf = tfidf_vectorizer.fit_transform(full_text_strings).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_text_strings).toarray()\n",
    "\n",
    "X_full_svd = svd.fit_transform(X_full_tfidf)\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "X_full_raw = np.hstack([X_full_svd])\n",
    "X_test_raw = np.hstack([X_test_svd])\n",
    "\n",
    "X_full_std = scaler.fit_transform(X_full_raw)\n",
    "X_test_std = scaler.transform(X_test_raw)\n",
    "\n",
    "X_full_tensor = torch.tensor(X_full_std, dtype=torch.float32)\n",
    "y_full_tensor = torch.tensor(full_labels, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_std, dtype=torch.float32)\n",
    "\n",
    "full_dataset_mlp = TensorDataset(X_full_tensor, y_full_tensor)\n",
    "full_loader_mlp = DataLoader(full_dataset_mlp, batch_size=64, sampler=create_weighted_sampler(full_domains, full_labels))\n",
    "test_loader_mlp = DataLoader(TensorDataset(X_test_tensor), batch_size=64)\n",
    "\n",
    "mlp_model = MLPClassifier(input_dim=X_full_tensor.shape[1], hidden_dim=256, dropout=0.5).to(device)\n",
    "train(mlp_model, full_loader_mlp, epochs=6, lr=1e-3, device=device)\n",
    "\n",
    "# === Full BiLSTM Training ===\n",
    "full_texts_padded = pad_sequence([torch.tensor(t) for t in full_texts], batch_first=True, padding_value=0)\n",
    "full_dataset_bilstm = TensorDataset(full_texts_padded, torch.tensor(full_labels, dtype=torch.float32))\n",
    "full_loader_bilstm = DataLoader(full_dataset_bilstm, batch_size=32, sampler=create_weighted_sampler(full_domains, full_labels))\n",
    "\n",
    "bilstm_model = BiLSTMClassifier(\n",
    "    vocab_size=17120,\n",
    "    hidden_dim=128,\n",
    "    num_layers=1,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "train(bilstm_model, full_loader_bilstm, epochs=5, lr=0.0005, device=device)\n",
    "\n",
    "# === Full CNN Training ===\n",
    "full_texts_padded = pad_sequence([torch.tensor(t) for t in full_texts], batch_first=True, padding_value=0)\n",
    "full_dataset_cnn = TensorDataset(full_texts_padded, torch.tensor(full_labels, dtype=torch.float32))\n",
    "full_loader_cnn = DataLoader(full_dataset_cnn, batch_size=32, sampler=create_weighted_sampler(full_domains, full_labels))\n",
    "\n",
    "cnn_model = CNNTextClassifier(\n",
    "    vocab_size=17120,\n",
    "    embedding_dim=128,\n",
    "    num_filters=50,\n",
    "    filter_sizes=(2, 3, 4),\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "train(cnn_model, full_loader_cnn, epochs=4, lr=0.001, device=device)\n",
    "\n",
    "test_dataset_cnn = TensorDataset(test_texts)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=32)\n",
    "\n",
    "# === Step 8: Predict on Test and Use Meta-Classifier ===\n",
    "bilstm_test_preds, bilstm_test_probs = predict_with_confidence(bilstm_model, test_loader_cnn, device=device)\n",
    "mlp_test_preds, mlp_test_probs = predict_with_confidence(mlp_model, test_loader_mlp, device=device)\n",
    "cnn_test_preds, cnn_test_probs = predict_with_confidence(cnn_model, test_loader_cnn, device=device)\n",
    "\n",
    "cnn_test_probs = np.array(cnn_test_probs)\n",
    "mlp_test_probs = np.array(mlp_test_probs)\n",
    "bilstm_test_probs = np.array(bilstm_test_probs)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"id\": list(range(len(cnn_test_preds))),\n",
    "    \"label\": cnn_test_preds\n",
    "}).to_csv(\"cnn_submission.csv\", index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"id\": list(range(len(mlp_test_preds))),\n",
    "    \"label\": mlp_test_preds\n",
    "}).to_csv(\"mlp_submission.csv\", index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"id\": list(range(len(bilstm_test_preds))),\n",
    "    \"label\": bilstm_test_preds\n",
    "}).to_csv(\"bilstm_submission.csv\", index=False)\n",
    "\n",
    "X_meta_test = np.vstack([\n",
    "        cnn_test_probs,\n",
    "        (cnn_test_probs >= 0.5).astype(float),\n",
    "        np.abs(cnn_test_probs - 0.5),\n",
    "        mlp_test_probs,\n",
    "        (mlp_test_probs >= 0.5).astype(float),\n",
    "        np.abs(mlp_test_probs - 0.5),\n",
    "        bilstm_test_probs,\n",
    "        (bilstm_test_probs >= 0.5).astype(float),\n",
    "        np.abs(bilstm_test_probs - 0.5)\n",
    "    ]).T\n",
    "\n",
    "X_meta_test_tensor = torch.tensor(X_meta_test, dtype=torch.float32).to(device)\n",
    "meta_nn.eval()\n",
    "with torch.no_grad():\n",
    "    logits = meta_nn(X_meta_test_tensor)\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "ensemble_probs = probs\n",
    "ensemble_preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"id\": list(range(len(ensemble_preds))),\n",
    "    \"label\": ensemble_preds.astype(int),\n",
    "    \"confidence\": ensemble_probs.flatten()\n",
    "})\n",
    "\n",
    "# === Step 9: Save Predictions ===\n",
    "results_df.to_csv(\"ensemble_submission_final.csv\", index=False)\n",
    "results_df[[\"id\", \"label\"]].to_csv(\"ensemble_labels_final.csv\", index=False)\n",
    "print(\"Final ensemble predictions saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e5bcdfc1-a541-49de-afda-54edf07bb3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ensemble predictions saved.\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"id\": list(range(len(ensemble_preds))),\n",
    "    \"label\": ensemble_preds.astype(int),\n",
    "    \"confidence\": ensemble_probs.flatten()\n",
    "})\n",
    "\n",
    "# === Step 9: Save Predictions ===\n",
    "results_df.to_csv(\"ensemble_submission_final.csv\", index=False)\n",
    "results_df[[\"id\", \"label\"]].to_csv(\"ensemble_labels_final.csv\", index=False)\n",
    "print(\"Final ensemble predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0032b4e0-4c2d-414b-a10e-bac57761d600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 2259, 0: 1741})\n",
      "Counter({0: 2186, 1: 1814})\n",
      "Counter({0: 2310, 1: 1690})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(bilstm_test_preds))\n",
    "print(Counter(cnn_test_preds))\n",
    "print(Counter(mlp_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a23d9017-d312-4ff4-917d-19743612ea41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 2569, 0: 1431})\n",
      "Counter({0: 2230, 1: 1770})\n",
      "Counter({0: 2309, 1: 1691})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(bilstm_test_preds))\n",
    "print(Counter(cnn_test_preds))\n",
    "print(Counter(mlp_test_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
