{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580ab100-cd64-4111-858b-f87e0fe885cb",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd7ed5b-a209-41c7-ac99-6c26d4210303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324b253b-bf09-43cd-b2b2-f8931d496d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea98dd9b-bf51-42c4-a704-3704ce0aa1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, num_filters=100, filter_sizes=(3, 4, 5), dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)           # (batch_size, seq_len, emb_dim)\n",
    "        x = x.permute(0, 2, 1)          # (batch_size, emb_dim, seq_len)\n",
    "        convs = [F.relu(conv(x)) for conv in self.convs]  # list of (batch_size, num_filters, *)\n",
    "        pooled = [F.max_pool1d(c, kernel_size=c.size(2)).squeeze(2) for c in convs]\n",
    "        cat = torch.cat(pooled, dim=1)  # (batch_size, num_filters * len(filter_sizes))\n",
    "        dropped = self.dropout(cat)\n",
    "        logits = self.fc(dropped)\n",
    "        return logits.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2cd1a0-a73f-4fed-8e10-fd8e63ff2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(token_lists: List[List[int]]) -> List[str]:\n",
    "    return [\" \".join(map(str, tokens)) for tokens in token_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0f64e3-f5f6-4e3b-9ed0-7cac3d847a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_lines(path):\n",
    "    texts, labels = [], []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            texts.append(torch.tensor(ex['text'], dtype=torch.long))\n",
    "            labels.append(ex['label'])\n",
    "    return texts, labels\n",
    "\n",
    "def load_test_json(path):\n",
    "    texts = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            texts.append(torch.tensor(ex['text'], dtype=torch.long))\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75347502-119e-41a5-a260-d323637d2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embed_stats(token_lists: List[List[int]], embedding_matrix: np.ndarray) -> np.ndarray:\n",
    "    stats = []\n",
    "    for tokens in token_lists:\n",
    "        embedded = np.array([embedding_matrix[t] for t in tokens if t < len(embedding_matrix)])\n",
    "        if embedded.size == 0:\n",
    "            mean = np.zeros(embedding_matrix.shape[1])\n",
    "            std = np.zeros(embedding_matrix.shape[1])\n",
    "        else:\n",
    "            mean = embedded.mean(axis=0)\n",
    "            std = embedded.std(axis=0)\n",
    "        stats.append(np.concatenate([mean, std]))\n",
    "    return np.vstack(stats)\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4bc46f0-9590-4293-a902-5bd1cff585b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_sampler(domains, labels):\n",
    "    from collections import Counter\n",
    "    from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "    bucket_keys = list(zip(domains, labels))\n",
    "    bucket_counts = Counter(bucket_keys)\n",
    "    bucket_weights = {k: 1.0 / count for k, count in bucket_counts.items()}\n",
    "    sample_weights = torch.DoubleTensor([bucket_weights[k] for k in bucket_keys])\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae61815c-e2a6-4d66-981e-041788424687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_val_split(texts, labels, domains, val_size_per_group=60, random_state=42):\n",
    "    random.seed(random_state)\n",
    "    from collections import defaultdict\n",
    "    buckets = defaultdict(list)\n",
    "\n",
    "    for x, y, d in zip(texts, labels, domains):\n",
    "        buckets[(d, y)].append((x, y, d))\n",
    "\n",
    "    train, val = [], []\n",
    "    for key in buckets:\n",
    "        group = buckets[key]\n",
    "        random.shuffle(group)\n",
    "        n_val = min(val_size_per_group, len(group))\n",
    "        val.extend(group[:n_val])\n",
    "        train.extend(group[n_val:])\n",
    "\n",
    "    random.shuffle(train)\n",
    "    random.shuffle(val)\n",
    "    tx, ty, td = zip(*train)\n",
    "    vx, vy, vd = zip(*val)\n",
    "    return list(tx), list(ty), list(td), list(vx), list(vy), list(vd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "283937b9-54d6-4bc5-ac6b-33bbf821acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, lr, device=\"cpu\"):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    pos_weight = torch.tensor([\n",
    "        sum(1 for y in train_loader.dataset.tensors[1] if y == 0) /\n",
    "        sum(1 for y in train_loader.dataset.tensors[1] if y == 1)\n",
    "    ], device=device)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch} â€” Train Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a622eb6-7b55-47c8-a9ea-9a028979c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_confidence(model, dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    preds, probs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\", leave=False):\n",
    "            if len(batch) == 2:\n",
    "                X_batch, _ = batch  # Ignore labels\n",
    "            else:\n",
    "                X_batch = batch[0]\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            batch_probs = torch.sigmoid(logits).squeeze()\n",
    "\n",
    "            if batch_probs.ndim == 0:\n",
    "                batch_probs = batch_probs.unsqueeze(0)\n",
    "\n",
    "            batch_preds = (batch_probs >= 0.5).int().tolist()\n",
    "            preds.extend(batch_preds)\n",
    "            probs.extend(batch_probs.cpu().tolist())\n",
    "\n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d8a74a1-fab8-4f7b-8b37-88f3ba85bfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 â€” Train Loss: 0.1206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 â€” Train Loss: 0.0637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 â€” Train Loss: 0.0459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 â€” Train Loss: 0.0329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 â€” Train Loss: 0.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 â€” Train Loss: 0.0211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 â€” Train Loss: 0.1575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 â€” Train Loss: 0.0817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 â€” Train Loss: 0.0518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 â€” Train Loss: 0.0367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 â€” Train Loss: 0.1350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 â€” Train Loss: 0.0697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 â€” Train Loss: 0.0503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 â€” Train Loss: 0.0369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 â€” Train Loss: 0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 â€” Train Loss: 0.0204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 â€” Train Loss: 0.1627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 â€” Train Loss: 0.0826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 â€” Train Loss: 0.0609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 â€” Train Loss: 0.0453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final ensemble predictions saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# === Step 0: Load and Split Data ===\n",
    "d1_texts, d1_labels = load_json_lines(\"data/domain1_train_data.json\")\n",
    "d2_texts, d2_labels = load_json_lines(\"data/domain2_train_data.json\")\n",
    "texts = d1_texts + d2_texts\n",
    "labels = d1_labels + d2_labels\n",
    "domains = [0] * len(d1_labels) + [1] * len(d2_labels)\n",
    "\n",
    "test_texts_raw = load_test_json(\"data/test_data.json\")\n",
    "test_texts = pad_sequence(test_texts_raw, batch_first=True, padding_value=0)\n",
    "\n",
    "test_dataset_cnn = TensorDataset(test_texts)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=32)\n",
    "\n",
    "# Stratified validation split: 60 from each domain\n",
    "train_texts, train_labels, train_domains, val_texts, val_labels, val_domains = stratified_train_val_split(\n",
    "    texts, labels, domains, val_size_per_group=60\n",
    ")\n",
    "\n",
    "# Pad after splitting\n",
    "train_texts_padded = pad_sequence(train_texts, batch_first=True, padding_value=0)\n",
    "val_texts_padded = pad_sequence(val_texts, batch_first=True, padding_value=0)\n",
    "\n",
    "# === Step 1: Token Conversion and Text Processing ===\n",
    "train_text_strings = tokens_to_text(train_texts)\n",
    "val_text_strings = tokens_to_text(val_texts)\n",
    "full_text_strings = tokens_to_text(train_texts + val_texts)\n",
    "test_text_strings = tokens_to_text(test_texts_raw)\n",
    "\n",
    "# === Step 2: TF-IDF + SVD for MLP ===\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_text_strings).toarray()\n",
    "X_val_tfidf = tfidf_vectorizer.transform(val_text_strings).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_text_strings).toarray()\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "X_val_svd = svd.transform(X_val_tfidf)\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "embedding_layer = nn.Embedding(17120, 128, padding_idx=0)\n",
    "embedding_matrix = embedding_layer.weight.detach().cpu().numpy()\n",
    "\n",
    "train_embed_stats = compute_embed_stats(train_texts, embedding_matrix)\n",
    "val_embed_stats = compute_embed_stats(val_texts, embedding_matrix)\n",
    "test_embed_stats = compute_embed_stats(test_texts_raw, embedding_matrix)\n",
    "\n",
    "X_train_raw = np.hstack([X_train_svd, train_embed_stats])\n",
    "X_val_raw = np.hstack([X_val_svd, val_embed_stats])\n",
    "X_test_raw = np.hstack([X_test_svd, test_embed_stats])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train_raw)\n",
    "X_val_std = scaler.transform(X_val_raw)\n",
    "X_test_std = scaler.transform(X_test_raw)\n",
    "\n",
    "# === Step 3: Prepare MLP Datasets ===\n",
    "X_train_tensor = torch.tensor(X_train_std, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_std, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(val_labels, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_std, dtype=torch.float32)\n",
    "\n",
    "train_dataset_mlp = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset_mlp = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset_mlp = TensorDataset(X_test_tensor)\n",
    "\n",
    "train_loader_mlp = DataLoader(train_dataset_mlp, batch_size=64, sampler=create_weighted_sampler(train_domains, train_labels))\n",
    "val_loader_mlp = DataLoader(val_dataset_mlp, batch_size=64)\n",
    "test_loader_mlp = DataLoader(test_dataset_mlp, batch_size=64)\n",
    "\n",
    "# === Step 4: Train MLP on Train Set ===\n",
    "mlp_model = MLPClassifier(input_dim=X_train_tensor.shape[1], hidden_dim=256, dropout=0.5).to(device)\n",
    "train(mlp_model, train_loader_mlp, epochs=6, lr=1e-3, device=device)\n",
    "\n",
    "# === Step 5: Train CNN on Train Set ===\n",
    "\n",
    "\n",
    "train_dataset_cnn = TensorDataset(train_texts_padded, torch.tensor(train_labels, dtype=torch.float32))\n",
    "val_dataset_cnn = TensorDataset(val_texts_padded, torch.tensor(val_labels, dtype=torch.float32))\n",
    "test_dataset_cnn = TensorDataset(test_texts)\n",
    "\n",
    "train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=32, sampler=create_weighted_sampler(train_domains, train_labels))\n",
    "val_loader_cnn = DataLoader(val_dataset_cnn, batch_size=32)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=32)\n",
    "\n",
    "cnn_model = CNNTextClassifier(\n",
    "    vocab_size=17120,\n",
    "    embedding_dim=128,\n",
    "    num_filters=50,\n",
    "    filter_sizes=(2, 3, 4),\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "train(cnn_model, train_loader_cnn, epochs=4, lr=0.001, device=device)\n",
    "\n",
    "# === Step 6: Train Meta-Classifier on Validation Predictions ===\n",
    "mlp_val_preds, mlp_val_probs = predict_with_confidence(mlp_model, val_loader_mlp, device=device)\n",
    "cnn_val_preds, cnn_val_probs = predict_with_confidence(cnn_model, val_loader_cnn, device=device)\n",
    "\n",
    "X_meta_val = np.vstack([cnn_val_probs, mlp_val_probs]).T\n",
    "y_meta_val = np.array(val_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Define shallow neural net ensemble classifier\n",
    "class MetaNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "meta_nn = MetaNN().to(device)\n",
    "optimizer_meta = torch.optim.Adam(meta_nn.parameters(), lr=1e-3)\n",
    "criterion_meta = nn.BCEWithLogitsLoss()\n",
    "\n",
    "X_meta_val_tensor = torch.tensor(X_meta_val, dtype=torch.float32).to(device)\n",
    "y_meta_val_tensor = torch.tensor(y_meta_val, dtype=torch.float32).to(device)\n",
    "\n",
    "meta_nn.train()\n",
    "for epoch in range(1, 11):\n",
    "    optimizer_meta.zero_grad()\n",
    "    logits = meta_nn(X_meta_val_tensor)\n",
    "    loss = criterion_meta(logits, y_meta_val_tensor)\n",
    "    loss.backward()\n",
    "    optimizer_meta.step()\n",
    "    print(f\"[MetaNN] Epoch {epoch} â€” Loss: {loss.item():.4f}\")\n",
    "\n",
    "# === Step 7: Recompute Features and Retrain CNN and MLP on Full Set ===\n",
    "# Recreate datasets first\n",
    "full_texts_raw = train_texts + val_texts\n",
    "full_labels = train_labels + val_labels\n",
    "full_text_strings = tokens_to_text(full_texts_raw)\n",
    "\n",
    "# Refit TF-IDF and SVD\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_full_tfidf = tfidf_vectorizer.fit_transform(full_text_strings).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_text_strings).toarray()\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_full_svd = svd.fit_transform(X_full_tfidf)\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "full_embed_stats = compute_embed_stats(full_texts_raw, embedding_matrix)\n",
    "test_embed_stats = compute_embed_stats(test_texts, embedding_matrix)\n",
    "\n",
    "X_full_raw = np.hstack([X_full_svd, full_embed_stats])\n",
    "X_test_raw = np.hstack([X_test_svd, test_embed_stats])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_full_std = scaler.fit_transform(X_full_raw)\n",
    "X_test_std = scaler.transform(X_test_raw)\n",
    "\n",
    "X_full_tensor = torch.tensor(X_full_std, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_std, dtype=torch.float32)\n",
    "# Recreate padded dataset for CNN\n",
    "full_texts_padded = pad_sequence(train_texts + val_texts, batch_first=True, padding_value=0)\n",
    "full_labels = train_labels + val_labels\n",
    "\n",
    "# X_full_tensor already set above with recomputed features\n",
    "y_full_tensor = torch.tensor(full_labels, dtype=torch.float32)\n",
    "full_dataset_mlp = TensorDataset(X_full_tensor, y_full_tensor)\n",
    "full_loader_mlp = DataLoader(full_dataset_mlp, batch_size=64, sampler=create_weighted_sampler(train_domains + val_domains, full_labels))\n",
    "\n",
    "mlp_model = MLPClassifier(input_dim=X_full_tensor.shape[1], hidden_dim=256, dropout=0.5).to(device)\n",
    "train(mlp_model, full_loader_mlp, epochs=6, lr=1e-3, device=device)\n",
    "\n",
    "\n",
    "full_dataset_cnn = TensorDataset(full_texts_padded, torch.tensor(full_labels, dtype=torch.float32))\n",
    "full_loader_cnn = DataLoader(full_dataset_cnn, batch_size=32, sampler=create_weighted_sampler(train_domains + val_domains, full_labels))\n",
    "\n",
    "cnn_model = CNNTextClassifier(\n",
    "    vocab_size=17120,\n",
    "    embedding_dim=128,\n",
    "    num_filters=50,\n",
    "    filter_sizes=(2, 3, 4),\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "train(cnn_model, full_loader_cnn, epochs=4, lr=0.001, device=device)\n",
    "\n",
    "# === Step 8: Predict on Test and Use Meta-Classifier ===\n",
    "mlp_test_preds, mlp_test_probs = predict_with_confidence(mlp_model, test_loader_mlp, device=device)\n",
    "cnn_test_preds, cnn_test_probs = predict_with_confidence(cnn_model, test_loader_cnn, device=device)\n",
    "\n",
    "X_meta_test = np.vstack([cnn_test_probs, mlp_test_probs]).T\n",
    "meta_nn.eval()\n",
    "X_meta_test_tensor = torch.tensor(X_meta_test, dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = meta_nn(X_meta_test_tensor)\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "ensemble_probs = probs\n",
    "ensemble_preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "# === Step 9: Save Predictions ===\n",
    "results_df = pd.DataFrame({\n",
    "    \"id\": list(range(len(ensemble_preds))),\n",
    "    \"label\": ensemble_preds.astype(int),\n",
    "    \"confidence\": ensemble_probs\n",
    "})\n",
    "results_df.to_csv(\"ensemble_submission_final.csv\", index=False)\n",
    "results_df[[\"id\", \"label\"]].to_csv(\"ensemble_labels_final.csv\", index=False)\n",
    "print(\"âœ… Final ensemble predictions saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
